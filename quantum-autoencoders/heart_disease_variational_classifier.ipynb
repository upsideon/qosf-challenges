{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2765a4d",
   "metadata": {},
   "source": [
    "## A Variational Classifier for Heart Disease\n",
    "\n",
    "This project is based on the PennyLane variational classifier tutorial found [here](https://pennylane.ai/qml/demos/tutorial_variational_classifier.html). To shake things up and to not repeat the solutions presented in the tutorial, the Iris dataset will not be used. Instead, a classifier will be produced that predicts the presense of heart disease based on a set of key indicators.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset was originally taken from the annual CDC health status survey from 2020, but the current version was preprocessed by [Kamil Pytlak](https://github.com/kamilpytlak) and posted on [Kaggle](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease). If you find this dataset interesting and would like to use it in one of your own projects, please provide appropriate attribution to the CDC surveyors and Kamil for their work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e4494",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing\n",
    "\n",
    "First, we begin by loading the dataset from a CSV via Pandas and taking a look at the features available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d854ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartDisease               No\n",
      "BMI                      16.6\n",
      "Smoking                   Yes\n",
      "AlcoholDrinking            No\n",
      "Stroke                     No\n",
      "PhysicalHealth            3.0\n",
      "MentalHealth             30.0\n",
      "DiffWalking                No\n",
      "Sex                    Female\n",
      "AgeCategory             55-59\n",
      "Race                    White\n",
      "Diabetic                  Yes\n",
      "PhysicalActivity          Yes\n",
      "GenHealth           Very good\n",
      "SleepTime                 5.0\n",
      "Asthma                    Yes\n",
      "KidneyDisease              No\n",
      "SkinCancer                Yes\n",
      "Name: 0, dtype: object\n",
      "Number of Features: 17\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "data = pandas.read_csv(\"./data/heart_2020_cleaned.csv\")\n",
    "first_row = data.iloc[0]\n",
    "num_features = len(first_row) - 1\n",
    "\n",
    "print(first_row)\n",
    "print(f\"Number of Features: {num_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8744ff2d",
   "metadata": {},
   "source": [
    "There are a total of 18 features with 14 of them being categorical and each category represented as a string. Since the classifier will only take in numerical values, we need to map our categories to real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81aa46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartDisease         0.0\n",
      "BMI                 16.6\n",
      "Smoking              1.0\n",
      "AlcoholDrinking      0.0\n",
      "Stroke               0.0\n",
      "PhysicalHealth       3.0\n",
      "MentalHealth        30.0\n",
      "DiffWalking          0.0\n",
      "Sex                  0.0\n",
      "AgeCategory          7.0\n",
      "Race                 5.0\n",
      "Diabetic             2.0\n",
      "PhysicalActivity     1.0\n",
      "GenHealth            4.0\n",
      "SleepTime            5.0\n",
      "Asthma               1.0\n",
      "KidneyDisease        0.0\n",
      "SkinCancer           1.0\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = [\n",
    "    \"HeartDisease\",\n",
    "    \"Smoking\",\n",
    "    \"AlcoholDrinking\",\n",
    "    \"Stroke\",\n",
    "    \"DiffWalking\",\n",
    "    \"Sex\",\n",
    "    \"AgeCategory\",\n",
    "    \"Race\",\n",
    "    \"Diabetic\",\n",
    "    \"PhysicalActivity\",\n",
    "    \"GenHealth\",\n",
    "    \"Asthma\",\n",
    "    \"KidneyDisease\",\n",
    "    \"SkinCancer\",\n",
    "]\n",
    "\n",
    "for column in categorical_columns:\n",
    "    data[column] = data[column].astype(\"category\").cat.codes\n",
    "    \n",
    "first_row = data.iloc[0]\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aed17e",
   "metadata": {},
   "source": [
    "Now that our feature values are numerical, we will randomly select a subset of the dataset to reduce training time and then split that subset into training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b272de39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Subset Size: 1000\n",
      "Training Set Size: 800\n",
      "Validation Set Size: 100\n",
      "Testing Set Size: 100\n"
     ]
    }
   ],
   "source": [
    "data_subset_size = 1000\n",
    "training_set_fraction = 0.8\n",
    "validation_set_fraction = 0.1\n",
    "testing_set_fraction = 0.1\n",
    "\n",
    "training_end = int(data_subset_size * training_set_fraction)\n",
    "validation_end = int(data_subset_size * (training_set_fraction + validation_set_fraction))\n",
    "\n",
    "data = data.sample(n=data_subset_size)\n",
    "\n",
    "training_set = data.iloc[:training_end]\n",
    "validation_set = data.iloc[training_end:validation_end]\n",
    "testing_set = data.iloc[validation_end:]\n",
    "\n",
    "print(f\"Data Subset Size: {data_subset_size}\")\n",
    "print(f\"Training Set Size: {len(training_set)}\")\n",
    "print(f\"Validation Set Size: {len(validation_set)}\")\n",
    "print(f\"Testing Set Size: {len(testing_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f75d8",
   "metadata": {},
   "source": [
    "Before proceeding, we split the training, validation, and testing sets into labels and features. We also convert the labels from values in the set `{0, 1}` to values in the set `{-1, 1}`. Expected value measurements from our quantum circuit will be within the interval `[-1, 1]` and having our label values as the endpoints of this interval makes computing the cost function easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c55ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane import numpy as np\n",
    "\n",
    "label_column = \"HeartDisease\"\n",
    "\n",
    "training_labels = np.array(training_set[label_column].values, dtype=np.float64, requires_grad=True)\n",
    "validation_labels = np.array(validation_set[label_column].values, dtype=np.float64, requires_grad=True)\n",
    "testing_labels = np.array(testing_set[label_column].values, dtype=np.float64, requires_grad=True)\n",
    "\n",
    "training_features = np.array(training_set.drop(label_column, axis=1).values, requires_grad=True)\n",
    "validation_features = np.array(validation_set.drop(label_column, axis=1).values, requires_grad=True)\n",
    "testing_features = np.array(testing_set.drop(label_column, axis=1).values, requires_grad=True)\n",
    "\n",
    "training_labels[np.where(training_labels == 0)] = -1\n",
    "validation_labels[np.where(validation_labels == 0)] = -1\n",
    "testing_labels[np.where(testing_labels == 0)] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29e400",
   "metadata": {},
   "source": [
    "### Data Encoding\n",
    "\n",
    "Before using our dataset, we need to consider how to encode our feature vectors into quantum states. There are a number of ways of going about state preparation, but which is best for our use case?\n",
    "\n",
    "Below is a table taken from [Schuld and Petruccione (2018)](https://link.springer.com/book/10.1007/978-3-319-96424-9) which contains four encoding strategies for a dataset of M inputs with N features each:\n",
    "\n",
    "| Encoding| Number of Qubits | Runtime of State Prep | Input Features |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| Basis | N | O(MN) | Binary |\n",
    "| Amplitude | log(N) | O(MN)/O(log(MN))\\* | Continuous |\n",
    "| Qsample | N | O(2^N)/O(N)\\* | Binary |\n",
    "| Hamiltonian | log(N) | O(MN)/O(log(MN))\\*| Continuous|\n",
    "\n",
    "Note that the authors indicate that certain datasets or models can be encoded in the runtimes which have asterixes next to them. Looking at the table, we can immediately eliminate the basis and Qsample encoding strategies as they are only applicable to binary features. Although some of our features are binary, a great deal of them are continuous.\n",
    "\n",
    "Amplitude encoding and Hamiltonian encoding have the same orders for their space and time complexities, so if we were solely concerned with the number of qubits or runtime, either should work fine. We will use PennyLane's implementation of amplitude encoding within this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c1e3d2",
   "metadata": {},
   "source": [
    "### Variational Circuit Construction\n",
    "\n",
    "Now that we've preprocessed the dataset and decided on a data encoding strategy, the variational circuit can be constructed. We begin with importing the PennyLane package, the standard Python mathematics package, and NumPy. Our variational circuit will require a classical optimization strategy, so we also import PennyLane's implementation of the Nesterov momentum optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf6681a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "\n",
    "from pennylane import math\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c080c52",
   "metadata": {},
   "source": [
    "Next we define a series of constants which will define the structure of the circuit. These are the number of features, the number of layers, and the number of qubits (or wires in the parlance of PennyLane). Note that the feature count is determined from the dataset and that the number of layers is directly configurable. The number of qubits is determined by the rule that amplitude encoding requires $n$ qubits to encode $2^n$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5805b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = training_features.shape[1]\n",
    "num_layers = 2\n",
    "\n",
    "\"\"\"\n",
    "Amplitude encoding requires n qubits to encode 2^n features.\n",
    "Here we apply a base two logarithm via the base change formula to determine n.\n",
    "\"\"\"\n",
    "num_wires = int(math.ceil(math.log(num_features) / math.log(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d0a70",
   "metadata": {},
   "source": [
    "Knowing the number of qubits enables us to define a function for state preparation. The function uses the `AmplitudeEmbedding` function to amplitude encode the provided features within the circuit's qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daabe126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_preparation(features):\n",
    "    \"\"\"\n",
    "    Prepares a state by amplitude encoding the provided features.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    features - An instance of features from the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalizing features and encoding them with padding.\n",
    "    qml.AmplitudeEmbedding(\n",
    "        features=features,\n",
    "        wires=range(num_wires),\n",
    "        pad_with=0.,\n",
    "        normalize=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb345f7",
   "metadata": {},
   "source": [
    "At this point, data can be encoded into the circuit, but it is not operated upon. A variational circuit consists of a series of layers which carry out operations on quantum states based on a set of parameters. In machine learning, these parameters are known as weights. There are different ways to construct these layers, but here we take the approach of each set of three weights defining an arbitrary rotation applied to a qubit. Following the rotation, each qubit is entangled with one of its neighbors until there is a chain of entanglement passing through all qubits in a given layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "049e5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(layer_weights):\n",
    "    \"\"\"\n",
    "    Adds a layer to the variational circuit.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    weights - A matrix of weights which define the phi, theta,\n",
    "    and omega values for the circuit rotations.\n",
    "    \"\"\"\n",
    "    num_wires = layer_weights.shape[0]\n",
    "    \n",
    "    for wire_index, wire_weights in enumerate(layer_weights):\n",
    "        # Applying rotation on qubit.\n",
    "        qml.Rot(*list(wire_weights), wires=wire_index)\n",
    "        \n",
    "        # Entangling qubit with its neighbor, wrapping around at the end.\n",
    "        qml.CNOT(wires=[wire_index, (wire_index + 1) % num_wires])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b52ca13",
   "metadata": {},
   "source": [
    "A quantum device is initialized and the state preparation and layering elements are combined to produce a variational circuit. The circuit is executed and repeated measurements are taken in the Z measurement basis to determine the circuit's expected value for the given input. The expected value is treated as the circuit's response to the provided instance of our classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b614829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device('default.qubit', wires=num_wires)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(features, weights):\n",
    "    state_preparation(features)\n",
    "    \n",
    "    for layer_weights in weights:\n",
    "        layer(layer_weights)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99cdfbf",
   "metadata": {},
   "source": [
    "### Training a Variational Classifier\n",
    "\n",
    "A variational classifier can be created by calling the variational circuit with weights, a bias term, and a set of features to classify. The classifier is unlikely to do very well unless it has been trained to optimize its weights and bias. Below are functions for creating a classifier, returning its predictions, and evaluating those predictions in terms of loss. Loss is determined by the average squared difference between the true labels corresponding to a set of features and the classifier's predicted label for those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1dc9068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_classifier(weights, bias, features):\n",
    "    return circuit(features, weights) + bias\n",
    "\n",
    "def square_loss(labels, predictions):\n",
    "    return np.sum((labels - predictions) ** 2) / len(labels)\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "    num_correct = len(np.where(predictions == labels)[0])\n",
    "    return num_correct / len(labels)\n",
    "\n",
    "def predict(weights, bias, features):\n",
    "    predictions = np.array([np.sign(variational_classifier(weights, bias, f)) for f in features])\n",
    "    return predictions\n",
    "\n",
    "def cost(weights, bias, features, labels):\n",
    "    predictions = np.array([variational_classifier(weights, bias, f) for f in features])\n",
    "    return square_loss(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23d2fd",
   "metadata": {},
   "source": [
    "The training process begins with the initialization of the optimizer, setting the batch size, and setting the number of training iterations, known as epochs. The batch size is the number of instances which are randomly selected and classified during a given epoch. For example, if the batch size was `5`, then `5` instances would be classified per epoch.\n",
    "\n",
    "Next, a set of starting weights is randomly initialized and the bias term is set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88e43c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = NesterovMomentumOptimizer()\n",
    "batch_size = 50\n",
    "num_epochs = 30\n",
    "\n",
    "# A rotation is applied to every qubit and there are three parameters\n",
    "# for each rotation: phi, theta, and omega.\n",
    "weights = np.random.rand(num_layers, num_wires, 3, requires_grad=True)\n",
    "\n",
    "bias = np.array(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f9bf33",
   "metadata": {},
   "source": [
    "The core of the training process consists of randomly selecting batches of features, making predictions using those features, and then adjusting the weights and bias based on the classifiers performance. As the classifier goes through training iterations, the cost associated with its predictions declines, which results in a corresponding increase in accuracy on both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aca0a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:     1 | Cost: 1.5534128 | Acc train: 0.1462500 | Acc validation: 0.0900000 \n",
      "Epoch:     2 | Cost: 1.4097524 | Acc train: 0.1575000 | Acc validation: 0.1000000 \n",
      "Epoch:     3 | Cost: 1.2267578 | Acc train: 0.1750000 | Acc validation: 0.1400000 \n",
      "Epoch:     4 | Cost: 1.0355481 | Acc train: 0.3175000 | Acc validation: 0.3100000 \n",
      "Epoch:     5 | Cost: 0.8494911 | Acc train: 0.8862500 | Acc validation: 0.9300000 \n",
      "Epoch:     6 | Cost: 0.6852686 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:     7 | Cost: 0.5553814 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:     8 | Cost: 0.4596633 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:     9 | Cost: 0.3970745 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    10 | Cost: 0.3623222 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    11 | Cost: 0.3516563 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    12 | Cost: 0.3559749 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    13 | Cost: 0.3696567 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    14 | Cost: 0.3878293 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    15 | Cost: 0.4066116 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    16 | Cost: 0.4230051 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    17 | Cost: 0.4354312 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    18 | Cost: 0.4421084 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    19 | Cost: 0.4447286 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    20 | Cost: 0.4425996 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    21 | Cost: 0.4344603 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    22 | Cost: 0.4232259 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    23 | Cost: 0.4103675 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    24 | Cost: 0.3961825 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    25 | Cost: 0.3815703 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    26 | Cost: 0.3700311 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    27 | Cost: 0.3602819 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    28 | Cost: 0.3528935 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    29 | Cost: 0.3479278 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Epoch:    30 | Cost: 0.3449911 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
      "Testing Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Selecting batch features and labels.\n",
    "    batch_index = np.random.randint(0, len(training_features), (batch_size,))\n",
    "    training_batch_features = training_features[batch_index]\n",
    "    training_batch_labels = training_labels[batch_index]\n",
    "    \n",
    "    # Updating parameters using optimizer.   \n",
    "    weights, bias, _, _ = optimizer.step(\n",
    "        cost,\n",
    "        weights,\n",
    "        bias,\n",
    "        training_batch_features,\n",
    "        training_batch_labels,\n",
    "    )\n",
    "    \n",
    "    # Predict labels on training and validation sets.\n",
    "    training_predictions = predict(weights, bias, training_features)\n",
    "    validation_predictions = predict(weights, bias, validation_features)\n",
    "    \n",
    "    # Computing accuracy on training and validation sets.\n",
    "    training_accuracy = accuracy(training_labels, training_predictions)\n",
    "    validation_accuracy = accuracy(validation_labels, validation_predictions)\n",
    "    \n",
    "    print(\n",
    "        \"Epoch: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "        \"\".format(epoch + 1, cost(\n",
    "            weights,\n",
    "            bias,\n",
    "            training_features,\n",
    "            training_labels,\n",
    "        ), training_accuracy, validation_accuracy)\n",
    "    )\n",
    "\n",
    "testing_predictions = predict(weights, bias, testing_features)\n",
    "testing_accuracy = accuracy(testing_labels, testing_predictions)\n",
    "print(f\"Testing Accuracy: {testing_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d09fa",
   "metadata": {},
   "source": [
    "After the model has been trained, the weights and bias term can be saved off for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c68ac514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_filepath = \"models/heart_disease_weights.npy\"\n",
    "model_bias_filepath = \"models/heart_disease_bias.npy\"\n",
    "\n",
    "np.save(model_weights_filepath, weights)\n",
    "np.save(model_bias_filepath, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec74795",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61464885",
   "metadata": {},
   "source": [
    "In my experimentation, I was able to produce a model that has 90.6% accuracy on the training set, 93% accuracy on the validation set, and 97% accuracy on the test set. Assuming the test set is a decent generalization of the population, this means that the classifier will correctly identify heart disease for approximately 9 out of 10 individuals who answer the survey questions truthfully.\n",
    "\n",
    "The output from the best experiment is provided below:\n",
    "\n",
    "```\n",
    "Epoch:     1 | Cost: 1.5534128 | Acc train: 0.1462500 | Acc validation: 0.0900000 \n",
    "Epoch:     2 | Cost: 1.4097524 | Acc train: 0.1575000 | Acc validation: 0.1000000 \n",
    "Epoch:     3 | Cost: 1.2267578 | Acc train: 0.1750000 | Acc validation: 0.1400000 \n",
    "Epoch:     4 | Cost: 1.0355481 | Acc train: 0.3175000 | Acc validation: 0.3100000 \n",
    "Epoch:     5 | Cost: 0.8494911 | Acc train: 0.8862500 | Acc validation: 0.9300000 \n",
    "Epoch:     6 | Cost: 0.6852686 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:     7 | Cost: 0.5553814 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:     8 | Cost: 0.4596633 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:     9 | Cost: 0.3970745 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    10 | Cost: 0.3623222 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    11 | Cost: 0.3516563 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    12 | Cost: 0.3559749 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    13 | Cost: 0.3696567 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    14 | Cost: 0.3878293 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    15 | Cost: 0.4066116 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    16 | Cost: 0.4230051 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    17 | Cost: 0.4354312 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    18 | Cost: 0.4421084 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    19 | Cost: 0.4447286 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    20 | Cost: 0.4425996 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    21 | Cost: 0.4344603 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    22 | Cost: 0.4232259 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    23 | Cost: 0.4103675 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    24 | Cost: 0.3961825 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    25 | Cost: 0.3815703 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    26 | Cost: 0.3700311 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    27 | Cost: 0.3602819 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    28 | Cost: 0.3528935 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    29 | Cost: 0.3479278 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Epoch:    30 | Cost: 0.3449911 | Acc train: 0.9062500 | Acc validation: 0.9300000 \n",
    "Testing Accuracy: 0.97\n",
    "```\n",
    "\n",
    "The model weights and bias values are located [here](models/heart_disease_weights_final.npy) and [here](models/heart_disease_bias_final.npy) respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
